# solve with JuMP, to double check our implementation

using JuMP
using Ipopt
# using GLPK

function get_solution_quadratic(Q, q, A, b, xâ‚€)
    model = Model()
    set_optimizer(model, Ipopt.Optimizer)

    @variable(model, x[1:size(xâ‚€, 1)])
    @objective(model, Min, 0.5*x'*Q*x + q'*x)
    objective_function(model, QuadExpr)
    @constraint(model, con, A*x .<= b )

    optimize!(model)

    return value.(x)
end

function check_nonempty_constraints(ð”“, xâ‚€)
    Q, q, l, u, E, b = (ð”“.Q, ð”“.q, ð”“.l, ð”“.u, ð”“.E, ð”“.b)
    model = Model()
    set_optimizer(model, Ipopt.Optimizer)

    @variable(model, x[1:size(xâ‚€, 1)])
    @objective(model, Min, (E*x-b)'*(E*x-b))
    objective_function(model, QuadExpr)
    #@constraint(model, con, E*x .== b )
    @constraint(model, con2, l .<= x .<= u)

    optimize!(model)

    return value.(x)
end

function get_solution_quadratic_box_constrained(ð”“, xâ‚€)
    Q, q, l, u, E, b = (ð”“.Q, ð”“.q, ð”“.l, ð”“.u, ð”“.E, ð”“.b)
    model = Model()
    set_optimizer(model, Ipopt.Optimizer)

    @variable(model, x[1:size(xâ‚€, 1)])
    @objective(model, Min, 0.5*x'*Q*x + q'*x)
    objective_function(model, QuadExpr)
    @constraint(model, con, E*x .== b)
    @constraint(model, con2, l .<= x .<= u)

    optimize!(model)

    return value.(x)
end


"""
Examples:
```julia
subgradient.Î±=1.0;  # Adagrad + RMSProp + NesterovMomentum
#subgradient.Î³=0.9;  # RMSProp
algorithm.max_iter = 4000;
algorithm.Î¼â‚€ = rand(1000);
Ls = [];
is = [];
for i in 1:40
    run!(test);
    push!(Ls, test.result.memoria["Lâ€²"]...);
    push!(is,((i-1)*algorithm.max_iter .+ test.result.memoria["iâ€²"])...);
    set!(algorithm, test.result);
    algorithm.Î¼â‚€ = test.result.result["Î¼â€²"]
    algorithm.stopped = false;
    subgradient.Î± /= 2.0    # Adagrad + RMSProp
    #subgradient.Î³ = 1.0-(1.0-subgradient.Î³)/4.0
end

for i in 1:25
    run!(test);
    push!(Ls, test.result.memoria["Lâ€²"]...);
    push!(is,((i-1)*algorithm.max_iter .+ test.result.memoria["iâ€²"])...);
    set!(algorithm, test.result);
    algorithm.Î¼â‚€ = test.result.result["Î¼â€²"]
    algorithm.stopped = false;
    subgradient.Î± /= 3.0    # Adagrad + RMSProp
    subgradient.Î³ = 1.0-(1.0-subgradient.Î³)/3.0
end
```

"""
