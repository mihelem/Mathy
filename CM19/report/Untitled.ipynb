{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's this?\n",
    "\n",
    "In this package you'll find some experiments in optimization sprout from the realisation of a project for the course Computational Mathematics for Machine Learning and Data Analysis, held at the University of Pisa.\n",
    "For additional details, scroll through the Julia code. As a by side, please be mindful of the fact that this is the first time we get in touch with both Julia and mathematical programming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Copy the repo to your favourite folder, then add the path to the `LOAD_PATH` variable in your Julia REPL. \n",
    "This can be accomplished with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push!(LOAD_PATH, \"/path/to/the/repo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make it permanent by adding the command to `~/.julia/config/startup.jl`\n",
    "\n",
    "### Usage\n",
    "\n",
    "Load the package with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Subgradient with Automatic Parameter Tuning\n",
    "\n",
    "In this example you'll choose a subgradient step update and nest it inside the parameter tuner, together with the parameter searcher, in this case the standard Nelder Mead. Hence you'll generate a random quadratic Min Cost Flow  boxed separable problem (`QMCFBProblem`), run the algorithm and finally plot some data from the execution. We'll guide you through the process line by line.\n",
    "\n",
    "_Disclaimer_: this is an experimental algorithm for parameter tuning, in its early infancy, plenty of defects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgradient = Subgradient.HarmonicErgodicPrimalStep(k=4, a=0.01, b=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you create an instance of a subgradient method, the `HarmonicErgodicPrimalStep`. \n",
    "\n",
    "You can find such method, and many other, in the file `subgradient.jl`. \n",
    "\n",
    "This algorithm is specific to dual problems; in fact its peculiarity is that it keeps a convex combination of the primal values corresponding to the dual ones, through a choice of the subgradients, and such combination is guaranteed to converge to the primal optimal solution. More details in the docstring above the `struct` definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = QMCFBPAlgorithmD1SG(\n",
    "   localization=subgradient,      # subgradient method of choice\n",
    "   verbosity=1,                   # verbosity level (WIP)\n",
    "   max_iter=1000,                 # maximum number of iterations\n",
    "   ε=1e-6,                        # \\varepsilon find the optimal solution with ε tolerance\n",
    "   ϵ=1e-12);                      # \\epsilon geometric extension of vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you create an instance of an algorithm taylored to solve `QMCFBProblem`s with the use of subgradient methods.\n",
    "You can find such method in the file `algorithm/QMCFBP_D1_SG.jl`.\n",
    "\n",
    "The `ϵ` specifies how vertices are expanded, so that they are reached if nearer than `ϵ`.\n",
    "\n",
    "Other members of the `struct` not shown here are:\n",
    "\n",
    "* `μ₀` : the starting point in the dual space; if unspecified is set to zeros\n",
    "* `stopped` : a boolean indicating if the algorithm is fresh new or not (so there should be no initialization). This is used especially when you want to _prosecute from the result of an algorithm execution_, which can be attained with `function set!(algorithm::QMCFBPAlgorithmD1SG, result::OptimizationResult{QMCFBProblem})`\n",
    "* `memorabilia` : when setting up a solver, it is possible to specify a set of `memoranda`, which contains the names of the variables to be tracked during the execution of the algorithm. _In `memorabilia` there is a list of the trackable variables in the algorithm._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = NelderMead();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the object we are using for the parameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "halgorithm = WithParameterSearch{QMCFBProblem, typeof(algorithm), NelderMead}(\n",
    "   algorithm=algorithm,\n",
    "   searcher=searcher,\n",
    "   objective=\"L′\",                        # objective to be used in comparisons between point of the simplex\n",
    "   cmp=(a, b)->a>b,                       # comparison to be used to which value of the objective is better\n",
    "   searcher_iter=10,                      # see below\n",
    "   algorithm_iter=10000,                  # see below\n",
    "   algorithm_iter_per_search=200,         # see below\n",
    "   δ=0.1,                                 # see below\n",
    "   param_ranges=Dict(:a=>[1e-8, 1.0], :b=>[1e-8, 1.0]));   # ranges of the parameters to be used as a start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our _hyper algorithm_, which is just a wrapper for the `algorithm` to be used by the `searcher` together with the dynamics that we want to use for the search. \n",
    "\n",
    "You can find it in the file `hyper.jl`.\n",
    "\n",
    "At the moment, a naive strategy is implemented, consisting in a percentage `δ` of algorithm iterations to be explored by the `searcher`, followed by an `1-δ` remaining iterations with the parameters chosen by the preceding search. \n",
    "\n",
    "All this is repeated until the total `algorithm_iter` are accounted for. \n",
    "\n",
    "As an example, in this case we have `algorithm_iter_per_search = 200`, so the search phase costs 200 iterations; they contitute `δ=0.1` of the total iterations per cycle, so there are 1800 iterations following the search with the parameters set by the search. Note that, from the computational cost point of view, the search will actually cost `algorithm_iter_per_search*searcher_iter = 2000` iterations.\n",
    "\n",
    "Finally, the cycle is repeated 5 times to reach the total number of iterations, `algorithm_iter=10000`.\n",
    "\n",
    "You see that `param_ranges`, which is a `Dict{Symbol, Array}`, is indicating the range to start with in the parameter search. If you desire to keep fixed some parameter, you can do it with\n",
    "`fixed_params = Dict( dictionary of parameters to value to be held fixed )`, as an example we could specify\n",
    "`fixed_params = Dict(:a=>0.5)` in case we wanted the `a` parameter to be 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_test(\n",
    "    halgorithm,    # algorithm to be set in the solver\n",
    "    m=100,         # number of arcs (rows of incidence matrix E)\n",
    "    n=200,         # number of edges (columns of incidence matrix E)\n",
    "    singular=30,   # dim ker Q, where Q is the diagonal semidefinite positive hessian of the obj\n",
    "    active=0);     # the generator will try to position the minimum so that there will be active active constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is generating a random problem. Multiple dispatch will drive you to the code in `mincostflow.jl`, thanks to the type of `halgorithm <: OptimizationAlgorithm{QMCFBProblem}`.\n",
    "\n",
    "If you do not provide a problem, as in this case, the problem is generated by the function `generate_quadratic_min_cost_flow_boxed_problem`. If you already have a problem `my_problem` to solve, you can specify it with\n",
    "\n",
    "`𝔓=my_problem`\n",
    "\n",
    "Scroll through the code to see how to reduce the problem, in case you want to consider separately each connected component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.solver.options.memoranda = Set([\"norm∂L′\", \"L′\",\"i′\", \"params_best\", \"result_best\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember of `memoranda`?\n",
    "\n",
    "`get_test` is returning an `OptimizationInstance{QMCFBProblem}`. You can look at the structure in the file `Optimization.jl`.\n",
    "\n",
    "Each `OptimizationInstance` contains a __problem__ (whose type is parametrizing the instance), a __solver__ and a __result__.\n",
    "\n",
    "Each solver contains an __algorithm__ and some __options__, which are the ones we are specifying here, setting the variables to be tracked during the execution of the algorithm. Note that we are setting the variables to be tracked globally, so both in the `halgorithm` and the `algorithm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run!(test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, guess it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ls = [x.memoria[\"L′\"] for x in test.result.memoria[\"result_best\"]];\n",
    "norm∂Ls = [x.memoria[\"norm∂L′\"] for x in test.result.memoria[\"result_best\"]];\n",
    "is = [x.memoria[\"i′\"] for x in test.result.memoria[\"result_best\"]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are extracting some data which we'd love to represent. If the resulting graph will surprise you, recall how the `halgorithm` is repeating a search step 10 times...\n",
    "\n",
    "What about the `is`? Since we are dealing with subgradient algorithms, and not descent ones, we are not driving toward a better objective value at each step, hence the `i\\prime` are recording at which step a new best value is found. Note the `\\prime` also to the right of `L` and `norm∂L`: the meaning is the same, they are the values corresponding to a best new point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(is, Ls);\n",
    "plot(vcat(Ls...));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line is plotting a superposition of the objective value along the iterations, where the iterator is resetted at each step of the `halgorithm`.\n",
    "\n",
    "The second line is showing the same but without the superposition, so in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(Dict{Symbol, Float64}.(test.result.memoria[\"params_best\"]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is plotting a graph of the parameters (`a` and `b`) through the `halgorithm` iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_hyper = test.result.result[\"L′\"]\n",
    "params = test.result.result[\"params_best\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are extracting part of the result, the best value for the lagrangian dual and the best parameter values at the last cycle of the `halgorithm` - the ones to be used in case you want to prosecute with a pure `algorithm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝔓 = test.problem;\n",
    "Q, q, l, u, E, b = (𝔓.Q, 𝔓.q, 𝔓.l, 𝔓.u, 𝔓.E, 𝔓.b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you are extracting the problem.\n",
    "\n",
    "#### Example:  Subgradient with Polyak step and Elliptic rank-1 linear operator\n",
    "\n",
    "We'll show you a little code snippets to test the Polyak step size with linear elliptic operator ( in file `subgradient.jl`). Such method is enlarging the angle span by the last 2 directions of search, bringing the ellipsis generated by them to a circle. The method is applied to a prototypical example, where Polyak step size are easier because the minimum is known.\n",
    "\n",
    "The purpose of this example is to give you finer control on the usage of the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Plots\n",
    "function test(;n=2, max_iter=100, mapping=x->x)\n",
    "    t = max.(1.0, 0.5 .+ rand(n))\n",
    "    f = x -> t' * abs.(x)\n",
    "    ∂f = x -> t .* sign.(x)\n",
    "    subgradient = Subgradient.PolyakEllipStepSize(f_opt=0.0)   # optimum is known\n",
    "    x = rand(n).-0.5\n",
    "    init!(subgradient, f, ∂f, x)\n",
    "    fs = Float64[]\n",
    "    for i in 1:max_iter\n",
    "        x′, α, sg = step!(subgradient, f, ∂f, x)\n",
    "        fs = [fs; f(x′)]\n",
    "        x = x′\n",
    "    end\n",
    "    Plots.plot([i for i in 1:length(fs)], mapping(fs))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, there are 3 main steps in the usage of subgradient methods:\n",
    "\n",
    "* `subgradient = ...` : object construction\n",
    "* `init!(subgradient, f, ∂f, x)`  : initialization\n",
    "* `step!(subgradient, f, ∂f, x)`  : a single step of the subgradient method\n",
    "\n",
    "If you give it a try, you'll see how fast the method is converging (well, it has been devised to overcome exactly such kind of problems...)\n",
    "\n",
    "#### Example: Quadratic programming in a box with projected cojugate gradient: long search + local search\n",
    "\n",
    "You'll see how to set up a convex quadratic problem for solution with projected conjugate gradient. This is used inside the conjugate gradient algorithm for `QMCFBProblem`s. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝔓₁ = MinQuadratic.MQBProblem(\n",
    "            Q,\n",
    "            q,\n",
    "            l,\n",
    "            u)\n",
    "instance = OptimizationInstance{MQBProblem}()\n",
    "algorithm = MQBPAlgorithmPG1(\n",
    "    localization=QuadraticBoxPCGDescent(),\n",
    "    verbosity=-1,\n",
    "    max_iter=3000,\n",
    "    ε=1e-8,         # required maximum error\n",
    "    ϵ₀=1e-12)       # \n",
    "set!(instance,\n",
    "    problem=𝔓₁,\n",
    "    algorithm=algorithm,\n",
    "    options=MQBPSolverOptions(),     # set here the options you need, or afterward...\n",
    "    solver=OptimizationSolver{MQBProblem}())\n",
    "run!(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see more of how it works, take a look at the submodule MinQuadratic, in the file `minquadratic.jl` for the general settings of the problem and at `MQBP_P_PG.jl` for the specific projected gradient algorithm.\n",
    "As usual, if you want to set some variable to track, you should specify it in the `memoranda` member of `instanve.solver.options`.\n",
    "\n",
    "#### Example: QMCFBProblem solution with conjugate gradient and exact search\n",
    "\n",
    "This method is still not working as we hoped for singular `Q`, further specific analysis should be carried on to find out what's going wrong. \n",
    "\n",
    "By now you should be able to foresee the lines of code needed to setup a `QMCFBProblem` and then an `QMCFBPAlgorithmD1D` object to solve it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = QMCFBPAlgorithmD1D(\n",
    "    descent=ConjugateGradientDescent(), \n",
    "    verbosity=1, \n",
    "    max_iter=1000, \n",
    "    ε=1e-6,      # max error required\n",
    "    ϵₘ=1e-12,    # epsilon up to which to approximate being inside, outside\n",
    "    cure_singularity=true)   # if true, approach iteratively the singular Q\n",
    "test = get_test(\n",
    "    algorithm, \n",
    "    m=1000, \n",
    "    n=2000, \n",
    "    singular=2);\n",
    "run!(test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: something of numerical analysis\n",
    "\n",
    "If you like numerical analysis, we collected also some of the algorithm that we have been writing in numerical analysis in the file `numerical.jl`. There was the plan to drive toward pseudospectra, singular values and pseudosv, but in the end, no time to do it.\n",
    "\n",
    "As an example, if you want to give a try to fast GMRES, as described in the exercise of the book of Trefethen-Baum on Numerical Analysis, just give a call to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMRES_naive(A, b, k, ϵ, ϵₐ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be advised that code there is really a stub, maybe better than nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
