\documentclass[twoside]{mfitjournal}
\usepackage{amsbib}
\usepackage{amsmath}
\usepackage{amssymb}
\clubpenalty=10000
\widowpenalty=10000
\newcommand{\MFITnumber}{1}
\newcommand{\MFITom}{12}
\newcommand{\MFITyear}{2020}
\newcommand{\ISSN}{ISSN 2079-6641}
\newcommand{\OISSN}{ISSN 2079-665X}
\newcommand{\DOI}{DOI: 10.18454/2079-6641-\MFITyear-\MFITom-\MFITnumber-\firstpage-\lastpage}  %класс статьи
\renewcommand{\firstpage}{5}
\renewcommand{\lastpage}{12}
%\engshortpartit{}
%\shortpartit{}
\SECC{}
\ESECC{}
\begin{document}
\udk{УДК 512.24}% код направления
\shorttitle{Duality in Quadratic Min Cost Flow}%
\author{484805$\cup$568235}
\shortauthor{484805+568235}
\institute{University of Pisa, Italy}
\email{E-mail:}{matricola484805@gmail.com}
\workabstract {We analyse the simplest dual approaches to linearly constrained quadratic Min Cost Flow problems}
\keywords{\it }
\rcopyr{484805$\cup$568235}
%\rdate{Поступила в редакцию 11.09.2010~г.}
%\revisiondate{В окончательном варианте: 11.09.2010}
\msc{MSC 18A32} %http://www.ams.org/msc/
\engtitle{noML09}
%\engshorttitle{Index of a subgroup finding and occurence problem}
\engauthor{484805$\cup$568235}
%\eauthor{}\engshortauthor{Goryushkin~A.\,P.}
\enginstitute{University of Pisa}
\engabstract{Optimization of quadratic positive semidefinite separable linear and box constrained objective functions from Min Cost Flow problems}
\engkeywords{\it Key words: optimization, algorithmic problem}
\email{E-mail:}{matricola484805@gmail.com}
\engcopyr{484805$\cup$568235}
%\engdate{Original article submitted 11.09.2010.}
%\engrevisiondate{Revision submitted: 11.09.2010}
\pagestyle{myheadings}
\maketitle
\engmaketitle
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE, RO]{\ISSN}
\fancyhead[RE]{484805$\cup$568235}
%\fancyhead[LO]{Нахождение индекса подгруппы и проблема вхождения}
\cfoot{\arabic{page}}

\newpage

\section*{$(P)$ problem}

We are going to explore some possible dual approaches to solve the Min Cost Flow box constrained convex quadratic separable problem $(P)$ defined by

\begin{gather*}
\nu_*(P) = \min_{x \in S_{(P)}} \frac{1}{2} x^\intercal Q x + q^\intercal x \\
\textrm{where} \qquad S_{(P)} \: :\quad E x = b \quad \cap \quad l \le x \le u
\end{gather*}

where $E$, of size $(m, n)$, is the node-arc incidence matrix of a directed graph, $Q::Diagonal \succcurlyeq 0$.

\section*{Prelude}
\subsubsection*{Connected components}
The first step may be to divide the problem into the many separated sub-problems which it may represent, even if it is not necessary for most of the algorithm. since $(P)$ is separable, we can split $(P)$ in the $(P_i)$'s corresponding to the connected components defined by the incidence matrix $E$, the number of which is given by dimension of the left kernel of $E$. For simplicity with $(P)$ we will denote one of such smaller connected sub-problems.
\subsubsection*{Playing with the incidence matrix}
In some of the analysed solution strategies, it is required to split the incidence matrix in a sub-matrix of maximum rank and the remaining part, $E = [E_B \; E_N]$, and then calculate $\Tilde{E}_B^-1$, $\Tilde{E}_B^{-1}\Tilde{E}_N$, $\Tilde{E}_B^{-1}b$, where the $\Tilde{}$ here is indicating the subsets of rows with full rank. One way of obtaining this is applying the pivoted Gaussian reduction to the matrix $[E_B\; E_N\; b\; I]$, which will give us also the left kernel of $E_B$ in the lowest rows of the transformed identity. Using sparse matrices this is already efficient, anyway there is an easy graph theoretical algorithm to get the same result. First of all, how to extract $E_B$? This translates to extracting a spanning tree for each connected component, from each of which we remove a vertex (alternative proof of the rank of $E$ in terms of connected components). What about the inverse? Translating the matrix operations corresponding to the identity $\Tilde{E}_B^{-1} \Tilde{E}_B = I$, we have that for each arc we need to take the nodes corresponding to the sub-tree of the (undirected) spanning tree which is starting from one of the nodes of the arc and which is not touching the removed vertex; a minus sign is needed when we traverse the arc in the opposite direction as the one specified in the graph. In particular it may be efficient to remove the root from which we calculated the spanning tree, so that we can record, with a bit per edge, the side going toward the removed vertex. As a corollary we have a fast heuristic to get toward the sparsest possible inverse: we may look for the spanning tree in which the fan out is maximum, and choose as root the node with maximum fan out. We can even devise an easy exact algorithm, in case the inverse will be used frequently: chosen a root, the spanning tree giving the sparsest inverse is the one given by a breath first search starting from the root, since each node is appearing in the inverse as many time as its distance in the spanning tree from the root. So a naive algorithm would be to calculate the BFS spanning tree from each node of the graph; the best one is exactly what is usually considered the worst from a memory usage point of view (in that it fills more the queue). Ordering the nodes and arcs as in the rooted spanning tree, the matrix is triangular superior. To calculate faster the product $E_B^{-1}A$ for any matrix A, we can walk backward through the BFS tree, starting from the leaves.

\subsubsection*{Strong duality}
TO BE WRITTEN

\section*{$(D1)$ dual equality constraints, primal box constraints}
Box constraints are easy, so we can try to dualise only the equality constraints:
\begin{gather*}
    \nu_*(P) = \min_{x \in S_{(D1)}} \max_{\mu} L(x, \mu) \quad\ge\quad \max_{\mu} \min_{x \in S_{(D1)}} L(x, \mu) = \nu^*(D1)  \\
   \textrm{where}\qquad  L(x, \mu) = \frac{1}{2} x^\intercal Q x + q^\intercal x + \mu^\intercal (E x - b)  \\
    \textrm{and}\qquad  S_{(D1)} \: :\quad l \le x \le u
\end{gather*}

Since the feasible space is a box and the target function is separable and convex, to find
\[
    L(\mu) = \min_{x \in S_{(D1)}} L(x, \mu) = L(\overline{x}, \mu)
\]
it is sufficient to solve for $x$ in
\[
    \nabla_x L(\overbar{x}, \mu) = Q \overbar{x} + q + E^\intercal\mu = 0
\]
and then project to the nearest side, component by component:
\[
    x = \max.(l, \min.(u, -Q^{-1}(q + E^\intercal\mu)))
\]
A few words are due for the coordinates corresponding to $\ker Q$; in fact, for $Q_{j j} = 0$, we should look for
\[
    \min_x (q + E^\intercal \mu)_j x_j \implies \begin{cases}
        (q + E^\intercal \mu)_j > 0 \implies x_j = l_j \\
        (q + E^\intercal \mu)_j < 0 \implies x_j = u_j \\
        x_j \in [l_j, u_j] \quad \textrm{otherwise}
    \end{cases}
\]
We do not need to treat this case in a special way, since Floating Points standard is providing us with both $\pm\infty$ and $NaN$ and the zeros in the diagonal of $Q$ are to be considered $+0$. With respect to the value of the Lagrangian $L(\mu)$ we are done.
We need to keep in mind that the specific value of $x_j$ that we choose in case of $NaN$ is equivalent to a choice of the supergradient $\partial_\mu L(\mu) = E x(\mu) - b$, where $x(\mu)$ can be considered a multivalued function. Such consideration is relevant in the next step, the quest for $\max_\mu L(\mu)$.

\subsubsection*{A possible algorithmic approach}
Given $\mu$ we know how to calculate $L(\mu)$ and also the supergradients $\partial L(\mu)$.
This is what we need to try to find $\max_\mu L(\mu)$ with a conjugate subgradient method. The function we are maximising is piecewise differentiable, so a strategy for an exact line search, given a direction $d$, is to calculate and sort all the $\alpha$'s for which $\mu + \alpha d$ is intersecting a separating hyperplane between the differentiable regions. Here the set of $x$ coordinate which will vary with $\alpha$ is changing; considering the set of coordinates which is varying with $\alpha$ in the present region, we have for them $x_i = -Q_{i i}^{-1}(q + E^\intercal (\mu + \alpha d))_i$, the rest of them will be just $l_i$ or $u_i$; we can try to set $d^\intercal E x(\alpha) - d^\intercal b = 0$ and check if the $\alpha$ satisfying the equation is still inside the region.
Special attention is to be paid to the $NaN$ corresponding to the regions without interior points coming out from the kernel of $Q$; in particular, in addition to the geometrical $\epsilon$ in each comparison, the $\alpha$ need to be sorted keeping into account also if they are opening or closing a region.

\section*{$(D2)$ dual equality constraints and box constraints}
We start from the Lagrangian
    \begin{gather*}
    L(x, \mu, \lambda) = \frac{1}{2}x^\intercal Q x + q^\intercal x + \mu^\intercal(Ex - b) + \lambda_u^\intercal(x-u) + \lambda_l^\intercal(l-x) \\
    \textrm{with} \qquad \lambda .\ge 0
    \end{gather*}
Since the linear coefficients of $x$ will determine the position of the minimum of $L$ for fixed dual parameters, a possible substitution to make clear the relevant degrees of freedom is
    \begin{gather*}
    L(x, \mu, \gamma, \lambda_u) = \frac{1}{2}x^\intercal Q x + (\gamma + q)^\intercal x - \mu^\intercal b + (E^\intercal\mu - \gamma)^\intercal l + \lambda_u^\intercal(l-u) \\
    \textrm{with} \qquad \lambda_u .\ge 0 \; \And \; \lambda_u + E^\intercal\mu - \gamma .\ge 0 \\
    \impliedby \qquad
    \gamma = \Delta \lambda + E^\intercal \mu \qquad \textrm{where} \qquad \Delta \lambda = \lambda_u - \lambda_l
    \end{gather*}
In the coordinate subspace defined by $\ker Q$ the Lagrangian is unbounded below unless $\nabla_x|_{\ker Q} L(x, \mu, \gamma, \lambda_u) = 0$.
Since $Q$ is diagonal, we can decompose the coordinate space in the kernel and the co-image by a simple partition of the coordinates, corresponding in obvious notations to $Q_0, x_0 \dots$ and $Q_1, x_1, \dots$. Hence, \emph{for the interesting (finite) minima of $L$ varying $x$}, we have
    \begin{gather*}
    Q \Bar{x}(\gamma) + \gamma + q = 0 \implies \\
    L(\mu, \gamma, \lambda_u) = L(\Bar{x}(\gamma), \mu, \gamma, \lambda_u) = \min_x L(x, \mu, \gamma, \lambda_u) = \dots \\
    \dots \; = -\frac{1}{2}(\gamma + q)_1^\intercal Q_1^{-1} (\gamma + q)_1 - \mu^\intercal b + (E^\intercal\mu - \gamma)^\intercal l + \lambda_u^\intercal(l-u) = \dots \\
    \dots \; = -\frac{1}{2}(\gamma + q)_1^\intercal Q_1^{-1} (\gamma + q)_1 - \mu^\intercal b - (\gamma + q)^\intercal l + (E^\intercal\mu + q)^\intercal l + \lambda_u^\intercal(l-u)
    \end{gather*}
Note that
    \begin{gather*}
    Q \Bar{x} + \gamma + q = 0 \quad \iff \quad \gamma_0 + q_0 = 0 \; \And \; Q_1 \Bar{x}_1 + \gamma_1 + q_1 = 0
    \end{gather*}
\subsubsection*{Toward a possible constrained program}
With the substitution $\Tilde{\gamma} = \gamma + q$, our objective function is written
    \begin{gather*}
    \nu_*(D2) = \max_{\Tilde{\gamma}, \mu, \lambda_u} L(\Tilde{\gamma}, \mu, \lambda_u) \quad \textrm{beholding} \\
    \lambda_u .\ge \max.(0, \Tilde{\gamma} - q - E^\intercal\mu) \quad \And \quad \Tilde{\gamma}_0 = 0    \\
    \textrm{where} \quad L(\Tilde{\gamma}, \mu, \lambda_u) = -\frac{1}{2} \Tilde{\gamma}_1^\intercal Q_1^{-1}\Tilde{\gamma}_1 - \mu^\intercal b - \Tilde{\gamma}^\intercal l + (E^\intercal\mu + q)^\intercal l + \lambda_u^\intercal(l-u)
    \end{gather*}
\subsubsection*{Duality in action}
In the last formula for the Lagrangian, the last term $\lambda_u^\intercal(l-u)$, since we are looking for the maximum, we know that $l-u .\ge 0$ and we have the constraint $\lambda_u .\ge \max.(0, \Tilde{\gamma} - q - E^\intercal\mu)$, can be restricted to an equality, leading to
\begin{gather*}
    \max_{\Tilde{\gamma}, \mu, \lambda_u} L(\Tilde{\gamma}, \mu, \lambda_u) = \max_{\Tilde{\gamma}, \mu} L(\tilde{\gamma}, \mu)  \quad \textrm{where} \\
    L(\tilde{\gamma}, \mu) = -\frac{1}{2} \Tilde{\gamma}_1^\intercal Q_1^{-1}\Tilde{\gamma}_1 - \mu^\intercal b - \Tilde{\gamma}^\intercal l + (E^\intercal\mu + q)^\intercal l + \max.(0, \Tilde{\gamma} - q - E^\intercal\mu)^\intercal(l-u) = \dots \\
    \dots \; = -\frac{1}{2} \Tilde{\gamma}_1^\intercal Q_1^{-1}\Tilde{\gamma}_1 - \mu^\intercal b -  (\Tilde{\gamma} - q - E^\intercal\mu)^\intercal(l + (u-l) .* \theta.(\Tilde{\gamma} - q - E^\intercal\mu))
\end{gather*}
where $a\theta(a) = \max(0, a)$. Since $Q_1$ is diagonal, we have
\begin{gather*}
    L(\tilde{\gamma}, \mu) = \sum_i L_i (\tilde{\gamma}, \mu) - \mu^\intercal b \quad \textrm{where} \\
    L_i (\tilde{\gamma}, \mu) = -\frac{1}{2} \Tilde{\gamma}_i Q_{ii}^{-1}\Tilde{\gamma}_i - (\Tilde{\gamma} - q - E^\intercal\mu)_i \begin{cases}
             l_i \quad \textrm{when} (\Tilde{\gamma} - q - E^\intercal\mu)_i \le 0 \\
             u_i \quad \textrm{when} (\Tilde{\gamma} - q - E^\intercal\mu)_i \ge 0
        \end{cases}
\end{gather*}
which is an unconstrained, continuous and piecewise differentiable formulation of the problem. Recall that $(\Tilde{\gamma} - q - E^\intercal\mu) = \Delta \lambda$.
The optimality conditions can be expressed in terms of the supergradient: we denote with $p_i$ the coordinate $l_i$ if $\Delta\lambda_i < 0$, $u_i$ if $\Delta\lambda_i > 0$ and a value in $[l_i, u_i]$ otherwise. Then, when the supergradient conditions
\[
    Q p + \Tilde{\gamma} = 0 \quad \And \quad E p - b = 0
\]
are met, we obtain
\[
L = -\mu^\intercal b + \frac{1}{2}p^\intercal Q p + (E^\intercal\mu + q)^\intercal p = \frac{1}{2}p^\intercal Q p + q^\intercal p
\]
Duality is there.
Is it unexpected that the whole unconstrained behaviour of coordinates is encoded in supergradients of lower dimensional sub-spaces in the dual space? No, as an example because of the slack conditions of KKT. Another point of view is the one of Legendre-Fenchel transform - keeping in mind that our expression for constraints is linear in the dual parameters, while strict convexity is usually required to attain biunivocity of the primal-dual transformation.

\section*{$(D3)$ reduced equality constraints, dual box constraints}
We start from the Lagrangian
\begin{gather*}
    L(x, \nu) = \frac{1}{2} x^\intercal Q x + q^\intercal x + \mu(Ex-b) + \lambda_u^\intercal(x-u) + \lambda_l^\intercal(l-x) \\
    \textrm{where $\nu = [\mu, \lambda_u, \lambda_l]$ and $\lambda .\ge 0$}
\end{gather*}
Since the problem is separable, $Q$ is diagonal, it is easy to divide the subspace where the problem is quadratic from the space where it is linear
\begin{gather*}
    L(x, \nu) = \frac{1}{2} x_1^\intercal Q_1 x_1 + q_1^\intercal x_1 + \mu(E_1 x_1-b) + \lambda_{u1}^\intercal(x_1 - u_1) + \lambda_{l1}^\intercal(l_1 - x_1) + \dots \\
    \dots + q_0^\intercal x_0 + \mu E_0 x_0 + \lambda_{u0}^\intercal(x_0 - u_0) + \lambda_{l0}^\intercal(l_0 - x_0)
\end{gather*}
so that
\begin{gather*}
    \Bar{x} \in \arg \min_x L(x, \nu) \And L(\Bar{x}, \nu) > -\infty \implies Q_1 \Bar{x}_1 + q_1 + E_1\mu + \lambda_{u1} - \lambda_{l1} = 0 \\
    \qquad \textrm{and} \qquad q_0 + E_0^\intercal\mu + \lambda_{u0} - \lambda_{l0} = 0
\end{gather*}
The first equation gives us
\begin{gather*}
    \Bar{x}_1 = -Q_1^{-1}(q_1 + E_1^\intercal\mu + \lambda_{u1} - \lambda_{l1})
\end{gather*}
while the second one is a constraint describing the subset of the dual space where we can restrict our search of $\max_\nu L(\Bar{x}(\nu), \nu)$,
\begin{gather*}
    (0 \le) \lambda_{l0} = q_0 + E_0^\intercal\mu + \lambda_{u0}
\end{gather*}
With such substitutions, and keeping track of the consequently transformed inequality constraints,
\begin{gather*}
    L(\Bar{x}(\nu), \nu) = L_2(\Bar{x}(\nu), \nu) + L_1(\Bar{x}(\nu), \nu) + L_0 \qquad \textrm{where} \\
    L_2(\Bar{x}(\nu), \nu) = -\frac{1}{2}(E_1^\intercal\mu + \lambda_{u1}-\lambda_{l1})^\intercal Q_1^{-1} (E_1^\intercal\mu + \lambda_{u1}-\lambda_{l1}) \\
    L_1(\Bar{x}(\nu), \nu) = (l_0^\intercal E_0^\intercal - b^\intercal - q_1^\intercal Q^{-1}_1 E_1^\intercal )\mu + (-q_1^\intercal Q_1^{-1} - u_1^\intercal)\lambda_{u1} + \quad \dots \\
    \dots \qquad + (q_1^\intercal Q_1^{-1} + l_1^\intercal)\lambda_{l1} + (l_0 - u_0)^\intercal\lambda_{u0} \\
    L_0 = -\frac{1}{2}q_1^\intercalQ_1^{-1}q_1 + q_0^\intercal l_0 \\
    \textrm{with } \quad \lambda \ge 0 \; \And \; q_0 + E_0^\intercal\mu + \lambda_{u0} \ge 0
\end{gather*}


\begin{gather*}
L(\nu) = L_0 + L_1(\nu) +
\end{gather*}
Introducing the variables $\Delta \lambda = \lambda_u - \lambda_l$ and $\gamma = \Delta\lambda_1 + E_1^\intercal\mu$ we can write for the Lagrangian

\section*{Toward algorithms}
\subsection*{Smoothing}
$O(\sqrt{\frac{L}{\epsilon}})$
We can choose $L = O(\frac{1}{\epsilon})$, so attaining
$O(\frac{1}{\epsilon})$
\subsection*{Parameter Search}
From the first runs of the algorithms, it was evident that parameter tuning was
crucial for (deflected) subgradient algorithms.
A first tentative approach to parameter search could be
to implement automatic program differentiation and use then an hyper-algorithm,
automatically updating the parameters along the gradient (leading to a lower
target value). We still have to takle the implementation of such an idea.
A second approach could be to use a 0th order method, like the Nelder Mead simplex
algorithm, applied progressively to small run with different parameters;
the search is to be repeated during the execution of the main algorithm since the
convergence dynamics usually reveal variable optimal parameter
\subsubsection*{Experimental Results}
\subsubsubsection*{Nesterov Momentum}
\subsubsubsection*{AdaGrad (diagonal)}
\subsubsubsection*{RMSProp}
\subsubsubsection*{}


\end{document}
